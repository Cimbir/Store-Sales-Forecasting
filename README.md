# ML Final Project: Store Sales Forecasting


https://www.kaggle.com/competitions/walmart-recruiting-store-sales-forecasting

კონკურში გვევალება ამოვხსნათ Time Series ტიპის ამოცანა. Dataset-ის სახით გადმოგვეცემა თითოეული (Store, Dept) წყვილისათვის Weekly_Sales მიმდევრობა 2010 წლიდან 2011 წლამდე. Weekly_Sales ხასიათდება ყოველწლიური სეზონურობით. ამოცანის მიზანია, რომ გამოვიცნოთ თუ როგორი იქნება Weekly_Sales პარამეტრი შემდეგი 2 წლის განმავლობაში. თითოეულ სტრიქონს dataset-ში დართული აქვს IsHoliday პარამეტრი და ასევე შეგვიძლია გამოვიყენოთ სხვა დამატებითი features `features.csv` და `stores.csv` ფაილებიდან.

კონკურსი ფასდება WMAE მეტრიკით, რაც მსგავსია MAE მეტრიკის, ოღონდ ზოგიერთ მონაცემს, ამ შემთხვევაში `IsHoliday==true` ტიპის მონაცემებს მინიჭებული აქვს წონა 5, დანარჩენებს კი წონა 1.

# Training

ამოცანის ამოსახსნელად გამოვცადეთ რამდენიმე სხვადასხვა მოდელი. თითოეულს გასაწვრთნელად სჭირდება განსხვავებული ტიპის მიდგომა როგორც Feature Selection, Feature Engineering დროს, ასევე თვითონ training პროცესში. შესაბამისად თითოეულ მოდელს განვიხილავთ ცალკე.

ზოგი მოდელის ექსპერიმენტები დალოგილია MLFlow-ზე Dagshub-ის საშუალებით, ზოგი კი Wandb-ზე.

## XGBoost

https://dagshub.com/Cimbir/Store-Sales-Forecasting.mlflow/#/experiments/4

ნოუთბუქი: `model_experiment_XGBoost.ipynb`

ამ მოდელის გაწვრთნისას ვცდიდი თუ რამდენად კარგად იმუშავებდა სხვადასხვა features და შესაბამისად მჭირდებოდა სხვადასხვა ექსპერიმენტისათვის განსხვავებული Preprocessor-ები, რომლებიც პირდაპირაა MLflow-ზე დალოგილი `XGBoost_Preprocessor` პრეფიქსით.

ამ ექსპერიმენტებში გამოვიყენე ის ფაქტი, რომ XGBoost-ს თავად შეუძლია რელევანტური feature-ების შერჩევა XGBoost ხეების ასაგებად და ყველა მონაცემთა ბაზა `stores.csv`, `features.csv` და `train.csv` დავმერჯე ერთმანეთზე.

Train/Validation მონაცემების გამოსაყოფად გამოვიყენე `SPLIT_DATE=2011-09-01` თარიღი და გაჭრილი მონაცემებით ავაგე გრაფიკი, რომელიც ნოუთბუქშია ნაჩვენები.

### Feature Engineering 1

ექსპერიმენტები დავიწყე ძალიან მარტივი feature-ების შექმნით: `Year`, `Month`, `Day`. ფაქტობრივად 1 სტრინგად ჩაწერილი `Date` feature დავშალე 3 რიცხვით feature-ად. იმედი მქონდა, რომ ეს feature-ები დაეხმარებოდა მოდელს პერიოდულობის აღმოჩენაში. ასევე იმედს ვიქონიებდი, რომ (Store, Dept) წყვილით შეძლებდა განესხვავებინა სხვადასხვა Time Series და თითოეულისათვის გაეკეთებინა სწორი prediction.

XGBoost-ს საშუალება მივეცი თავად გამკლავებოდა categorical features, როგორიცაა `Type` და `na` მნიშვნელობები ჩემი მიერ დაწერილი preprocessing step-ის გარეშე.

გამოვიყენე low-level XGBoost API გაწვრთნის პროცესისთვის და არა scikit API, რომელიც ბევრის საშუალებას არ მაძლევდა. მაგალითად მჭირდებოდა რაიმე გზა, რომ XGBoost-ს პრიორიტეტი მიენიჭებინა IsHoliday==true მონაცემებისათვის. DMatrix საშუალებას მაძლევდა, რომ დამესეტა weight-ები თითოეული მონაცემისათვის

გამოვიყენე low-level XGBoost API გაწვრთნის პროცესისთვის და არა scikit API, რომელიც ბევრის საშუალებას არ მაძლევდა. მაგალითად მჭირდებოდა რაიმე გზა, რომ XGBoost-ს პრიორიტეტი მიენიჭებინა `IsHoliday==true` მონაცემებისათვის. `DMatrix` საშუალებას მაძლევდა, რომ დამესეტა `weight`-ები თითოეული მონაცემისათვის, რომლებიც გათვალისწინებული იქნებოდა Loss ფუნქციის მნიშვნელობაში.

### XGBoost Training 1

თავდაპირველი მნიშვნელოვანი ნაბიჯი იყო, რომ მეპოვა ამ ამოცანისათვის ყველაზე კარგი Loss Objective. თავდაპირველი ლოგიკური არჩევანი ვიფიქრე, რომ იქნებოდა MAE მეტრიკის გამოყენება Objective-ად, რომელიც დაიმპლემენტირებულია XGBoost-ის ახალ ვერსიაში `AbsoluteError`-ის სახელით. თუმცა გასათვალისწინებელია, რომ XGBoost-ს სჭირდება დაითვალოს პირველი რიგის წარმოებული და მეორე რიგის წარმოებული training-ის დროს, `AbsoluteError` კი 1 წერტილში საერთოდ არაა გაწარმოებადი და მეორე რიგის გრადიენტი კი 0-ის ტოლია. შესაბამისად შეიძლება, რომ საუკეთესო შედეგი არ დაედო `AbsoluteError`-ის გამოყენებას, ამიტომაც გადავწყვიტე ჩამეტარებინა ექსპერიმენტები.

### XGBoost_Obj_SquaredError_Estimator_1 - 200

https://dagshub.com/Cimbir/Store-Sales-Forecasting.mlflow/#/experiments/4/runs/d29087623d09491db75fdcc53adf4d80

ამ ექსპერიმენტების სერიაში `Estimator_1`-დან დაწყებული `Estimator_200`-ით დამთავრებული ვეძებდი რეკომენდირებული loss ფუნქცია `SquaredError`-ის baseline პერფორმანს, რათა შემდგომში შემედარებინა სხვა loss ფუნქციებისთვის.

თუ Estimator-ს ავიღებდი 1-ს, გასაკვირი არ არის, რომ მოდელი არ იყო საკმარისად კომპლექსური ამოცანისთვის და ვიღებდი მაღალი bias-ის მქონე underfitted მოდელს:

```
wmae_train 12099.414434121085
wmae_test 12441.135631336625
```

უკვე 10 Estimator-ზე ჩამოდიოდა ცდომლიება 6000-ის ფარგლებში, 50-ზე 3000-ის ფარგლებში, 100-200 Estimator-ზე კი უკვე რეგულარიზაციის გარეშე overfitted ხდებოდა მოდელი, როდესაც train 2000-ის ფარგლებში იყო, ხოლო test ისევ 3000-ის:

```
mae_train 2333.30269210183
mae_test 3369.795934797164
wmae_train 2350.272432798231
wmae_test 3505.5818113659448
```

### XGBoost_Obj_AbsoluteError_Estimator_1 - 200_HigherLR

https://dagshub.com/Cimbir/Store-Sales-Forecasting.mlflow/#/experiments/4/runs/c7f9a37a6b5c49b291cf8453f15cf288

`AbsoluteError` თავდაპირველად `Estimator=1`-ზე უკეთესი შედეგი დადო:

```
mae_train 11075.31378309588
mae_test 11305.03717124261
wmae_train 11248.186628292535
wmae_test 11582.631160385079
```

თუმცა რაც უფრო ვამატებდი `Estimator`-ებს, აშკარა ხდებოდა, რომ უფრო უჭირდა მონაცემებზე მორგება და overfit-იც კი ვიდრე `SquaredError` loss ფუნქციას.

`Estimator=100`-ზე მივიღე შემდეგი შედეგი:

```
wmae_train 3393.5045087223466
wmae_test 4106.367590955688
```

![Plot](plots/xgboost/SquaredError_Estimator_100.png)

ეს შევადაროთ `SquaredError`-ს `Estimator=100`-ზე:

```
wmae_train 2887.015479419683
wmae_test 3707.861059151612
```

![Plot](plots/xgboost/AbsoluteError_Estimator_100.png)

გრაფიკებიდანაც ჩანს, რომ `SquaredError` გაცილებით უკეთ აკეთებს prediction-ებს validation set-ზე, მიუხედავად იმისა, რომ ტოლი რაოდენობის XGBoost ხეები გვაქვს ორივე მოდელში.


შთაბეჭდილება დამრჩა, რომ იქიდან გამომდინარე, რომ მეორე რიგის გრადიენტი 0-ის ტოლი იყო უფრო ნელა სწავლობდა მოდელი, ამიტომაც learning rate-ის გაზრდა ვცადე `eta=0.2` პარამეტრით.

ამ ცდელობას გაცილებით უკეთესი შედეგი არ გამოუღია:

```
wmae_train 3649.768545723227
wmae_test 4218.194824268481
```

### XGBoost_Obj_HuberError_Estimator_100 - SlopeSearch

https://dagshub.com/Cimbir/Store-Sales-Forecasting.mlflow/#/experiments/4/runs/884d1229000847a6a51175318fbf76d6

ასევე ვცადე HuberLoss-ის გამოყენება, რომელიც ფაქტობრივად აერთიანებს MSE და MAE loss ფუნქციებს და delta პარამეტრი გადაწყვეტს, რომელი გამოიყენოს კონკრეტული მნიშვნელობებისათვის.

იდეაში ამ loss-ის გამოყენება კარგად ჟღერდა, თუმცა პრაქტიკაში MAE-ზეც და MSE-ზეც უფრო უარესი შედეგი აჩვენა.

GridSearch გავუშვი `huber_slope`-ზე იგივე `delta` პარამეტრზე მნიშვნელობებით `[0.7, 1.5, 2.5, 5, 10, 100, 1000]`. საუკეთესო ვარიანტად შეარჩია `huber_slope=10` შემდეგ ექსპერიმენტში:

https://dagshub.com/Cimbir/Store-Sales-Forecasting.mlflow/#/experiments/4/runs/1d66d95e0a424555a36e965f9578523d

თუმცა შედეგები მაინც არ იყო კარგი:

```
mae_train 4590.3573569983255
mae_test 5370.413544734716
wmae_train 4752.668478419167
wmae_test 5623.664094910361
```

ამ ექსპერიმენტებიდან საბოლოო ჯამში მივედი იმ დასკვნამდე, რომ XGBoost ყველაზე კარგად მუშაობს `SquaredError`-ის მინიმიზაციისას, თუნდაც WMAE მეტრიკა მაინტერესებდეს ყველაზე მეტად, ამიტომაც ამიერიდან ამ loss ფუნქციას გამოვიყენებ. თუ პრობლემა შეიქმნება WMAE-ს მინიმიზაციისას შემიძლია early stopping მექანიზმი ჩავურთო ყოველთვის.

