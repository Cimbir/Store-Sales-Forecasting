# ML Final Project: Store Sales Forecasting


https://www.kaggle.com/competitions/walmart-recruiting-store-sales-forecasting

კონკურში გვევალება ამოვხსნათ Time Series ტიპის ამოცანა. Dataset-ის სახით გადმოგვეცემა თითოეული (Store, Dept) წყვილისათვის Weekly_Sales მიმდევრობა 2010 წლიდან 2011 წლამდე. Weekly_Sales ხასიათდება ყოველწლიური სეზონურობით. ამოცანის მიზანია, რომ გამოვიცნოთ თუ როგორი იქნება Weekly_Sales პარამეტრი შემდეგი 2 წლის განმავლობაში. თითოეულ სტრიქონს dataset-ში დართული აქვს IsHoliday პარამეტრი და ასევე შეგვიძლია გამოვიყენოთ სხვა დამატებითი features `features.csv` და `stores.csv` ფაილებიდან.

კონკურსი ფასდება WMAE მეტრიკით, რაც მსგავსია MAE მეტრიკის, ოღონდ ზოგიერთ მონაცემს, ამ შემთხვევაში `IsHoliday==true` ტიპის მონაცემებს მინიჭებული აქვს წონა 5, დანარჩენებს კი წონა 1.

# Repository-ის სტრუქტურა

* **/plots** - დირექტორია, სადაც თითოეული მოდელისათვის საინტერესო plot-ებია მოთავსებული, რომლებსაც ამ README.md-ში ვიყენებთ
* **model_experiment_XGBoost.ipynb** - XGBoost მოდელის გაწვრთნის პროცესის კოდი თითოეული ნაბიჯით.
* **model_experiment_LightGBM.ipynb** - LightGBM მოდელის training.


# Training

ამოცანის ამოსახსნელად გამოვცადეთ რამდენიმე სხვადასხვა მოდელი. თითოეულს გასაწვრთნელად სჭირდება განსხვავებული ტიპის მიდგომა როგორც Feature Selection, Feature Engineering დროს, ასევე თვითონ training პროცესში. შესაბამისად თითოეულ მოდელს განვიხილავთ ცალკე.

ზოგი მოდელის ექსპერიმენტები დალოგილია MLFlow-ზე Dagshub-ის საშუალებით, ზოგი კი Wandb-ზე.

## XGBoost

https://dagshub.com/Cimbir/Store-Sales-Forecasting.mlflow/#/experiments/4

ნოუთბუქი: `model_experiment_XGBoost.ipynb`

ამ მოდელის გაწვრთნისას ვცდიდი თუ რამდენად კარგად იმუშავებდა სხვადასხვა features და შესაბამისად მჭირდებოდა სხვადასხვა ექსპერიმენტისათვის განსხვავებული Preprocessor-ები, რომლებიც პირდაპირაა MLflow-ზე დალოგილი `XGBoost_Preprocessor` პრეფიქსით.

ამ ექსპერიმენტებში გამოვიყენე ის ფაქტი, რომ XGBoost-ს თავად შეუძლია რელევანტური feature-ების შერჩევა XGBoost ხეების ასაგებად და ყველა მონაცემთა ბაზა `stores.csv`, `features.csv` და `train.csv` დავმერჯე ერთმანეთზე.

Train/Validation მონაცემების გამოსაყოფად გამოვიყენე `SPLIT_DATE=2011-09-01` თარიღი და გაჭრილი მონაცემებით ავაგე გრაფიკი, რომელიც ნოუთბუქშია ნაჩვენები.

### XGBoost Feature Engineering 1

ექსპერიმენტები დავიწყე ძალიან მარტივი feature-ების შექმნით: `Year`, `Month`, `Day`. ფაქტობრივად 1 სტრინგად ჩაწერილი `Date` feature დავშალე 3 რიცხვით feature-ად. იმედი მქონდა, რომ ეს feature-ები დაეხმარებოდა მოდელს პერიოდულობის აღმოჩენაში. ასევე იმედს ვიქონიებდი, რომ (Store, Dept) წყვილით შეძლებდა განესხვავებინა სხვადასხვა Time Series და თითოეულისათვის გაეკეთებინა სწორი prediction.

XGBoost-ს საშუალება მივეცი თავად გამკლავებოდა categorical features, როგორიცაა `Type` და `na` მნიშვნელობები ჩემი მიერ დაწერილი preprocessing step-ის გარეშე.

გამოვიყენე low-level XGBoost API გაწვრთნის პროცესისთვის და არა scikit API, რომელიც ბევრის საშუალებას არ მაძლევდა. მაგალითად მჭირდებოდა რაიმე გზა, რომ XGBoost-ს პრიორიტეტი მიენიჭებინა IsHoliday==true მონაცემებისათვის. DMatrix საშუალებას მაძლევდა, რომ დამესეტა weight-ები თითოეული მონაცემისათვის

გამოვიყენე low-level XGBoost API გაწვრთნის პროცესისთვის და არა scikit API, რომელიც ბევრის საშუალებას არ მაძლევდა. მაგალითად მჭირდებოდა რაიმე გზა, რომ XGBoost-ს პრიორიტეტი მიენიჭებინა `IsHoliday==true` მონაცემებისათვის. `DMatrix` საშუალებას მაძლევდა, რომ დამესეტა `weight`-ები თითოეული მონაცემისათვის, რომლებიც გათვალისწინებული იქნებოდა Loss ფუნქციის მნიშვნელობაში.

### XGBoost Training 1

თავდაპირველი მნიშვნელოვანი ნაბიჯი იყო, რომ მეპოვა ამ ამოცანისათვის ყველაზე კარგი Loss Objective. თავდაპირველი ლოგიკური არჩევანი ვიფიქრე, რომ იქნებოდა MAE მეტრიკის გამოყენება Objective-ად, რომელიც დაიმპლემენტირებულია XGBoost-ის ახალ ვერსიაში `AbsoluteError`-ის სახელით. თუმცა გასათვალისწინებელია, რომ XGBoost-ს სჭირდება დაითვალოს პირველი რიგის წარმოებული და მეორე რიგის წარმოებული training-ის დროს, `AbsoluteError` კი 1 წერტილში საერთოდ არაა გაწარმოებადი და მეორე რიგის გრადიენტი კი 0-ის ტოლია. შესაბამისად შეიძლება, რომ საუკეთესო შედეგი არ დაედო `AbsoluteError`-ის გამოყენებას, ამიტომაც გადავწყვიტე ჩამეტარებინა ექსპერიმენტები.

### XGBoost_Obj_SquaredError_Estimator_1 - 200

https://dagshub.com/Cimbir/Store-Sales-Forecasting.mlflow/#/experiments/4/runs/d29087623d09491db75fdcc53adf4d80

ამ ექსპერიმენტების სერიაში `Estimator_1`-დან დაწყებული `Estimator_200`-ით დამთავრებული ვეძებდი რეკომენდირებული loss ფუნქცია `SquaredError`-ის baseline პერფორმანს, რათა შემდგომში შემედარებინა სხვა loss ფუნქციებისთვის.

თუ Estimator-ს ავიღებდი 1-ს, გასაკვირი არ არის, რომ მოდელი არ იყო საკმარისად კომპლექსური ამოცანისთვის და ვიღებდი მაღალი bias-ის მქონე underfitted მოდელს:

```
wmae_train 12099.414434121085
wmae_test 12441.135631336625
```

უკვე 10 Estimator-ზე ჩამოდიოდა ცდომლიება 6000-ის ფარგლებში, 50-ზე 3000-ის ფარგლებში, 100-200 Estimator-ზე კი უკვე რეგულარიზაციის გარეშე overfitted ხდებოდა მოდელი, როდესაც train 2000-ის ფარგლებში იყო, ხოლო test ისევ 3000-ის:

```
mae_train 2333.30269210183
mae_test 3369.795934797164
wmae_train 2350.272432798231
wmae_test 3505.5818113659448
```

### XGBoost_Obj_AbsoluteError_Estimator_1 - 200_HigherLR

https://dagshub.com/Cimbir/Store-Sales-Forecasting.mlflow/#/experiments/4/runs/c7f9a37a6b5c49b291cf8453f15cf288

`AbsoluteError` თავდაპირველად `Estimator=1`-ზე უკეთესი შედეგი დადო:

```
mae_train 11075.31378309588
mae_test 11305.03717124261
wmae_train 11248.186628292535
wmae_test 11582.631160385079
```

თუმცა რაც უფრო ვამატებდი `Estimator`-ებს, აშკარა ხდებოდა, რომ უფრო უჭირდა მონაცემებზე მორგება და overfit-იც კი ვიდრე `SquaredError` loss ფუნქციას.

`Estimator=100`-ზე მივიღე შემდეგი შედეგი:

```
wmae_train 3393.5045087223466
wmae_test 4106.367590955688
```

![Plot](plots/xgboost/SquaredError_Estimator_100.png)

ეს შევადაროთ `SquaredError`-ს `Estimator=100`-ზე:

```
wmae_train 2887.015479419683
wmae_test 3707.861059151612
```

![Plot](plots/xgboost/AbsoluteError_Estimator_100.png)

გრაფიკებიდანაც ჩანს, რომ `SquaredError` გაცილებით უკეთ აკეთებს prediction-ებს validation set-ზე, მიუხედავად იმისა, რომ ტოლი რაოდენობის XGBoost ხეები გვაქვს ორივე მოდელში.


შთაბეჭდილება დამრჩა, რომ იქიდან გამომდინარე, რომ მეორე რიგის გრადიენტი 0-ის ტოლი იყო უფრო ნელა სწავლობდა მოდელი, ამიტომაც learning rate-ის გაზრდა ვცადე `eta=0.2` პარამეტრით.

ამ ცდელობას გაცილებით უკეთესი შედეგი არ გამოუღია:

```
wmae_train 3649.768545723227
wmae_test 4218.194824268481
```

### XGBoost_Obj_HuberError_Estimator_100 - SlopeSearch

https://dagshub.com/Cimbir/Store-Sales-Forecasting.mlflow/#/experiments/4/runs/884d1229000847a6a51175318fbf76d6

ასევე ვცადე HuberLoss-ის გამოყენება, რომელიც ფაქტობრივად აერთიანებს MSE და MAE loss ფუნქციებს და delta პარამეტრი გადაწყვეტს, რომელი გამოიყენოს კონკრეტული მნიშვნელობებისათვის.

იდეაში ამ loss-ის გამოყენება კარგად ჟღერდა, თუმცა პრაქტიკაში MAE-ზეც და MSE-ზეც უფრო უარესი შედეგი აჩვენა.

GridSearch გავუშვი `huber_slope`-ზე იგივე `delta` პარამეტრზე მნიშვნელობებით `[0.7, 1.5, 2.5, 5, 10, 100, 1000]`. საუკეთესო ვარიანტად შეარჩია `huber_slope=10` შემდეგ ექსპერიმენტში:

https://dagshub.com/Cimbir/Store-Sales-Forecasting.mlflow/#/experiments/4/runs/1d66d95e0a424555a36e965f9578523d

თუმცა შედეგები მაინც არ იყო კარგი:

```
mae_train 4590.3573569983255
mae_test 5370.413544734716
wmae_train 4752.668478419167
wmae_test 5623.664094910361
```

ამ ექსპერიმენტებიდან საბოლოო ჯამში მივედი იმ დასკვნამდე, რომ XGBoost ყველაზე კარგად მუშაობს `SquaredError`-ის მინიმიზაციისას, თუნდაც WMAE მეტრიკა მაინტერესებდეს ყველაზე მეტად, ამიტომაც ამიერიდან ამ loss ფუნქციას გამოვიყენებ. თუ პრობლემა შეიქმნება WMAE-ს მინიმიზაციისას შემიძლია early stopping მექანიზმი ჩავურთო ყოველთვის.

### XGBoost_Estimator_200_L1_100_L2_100_Depth_3

https://dagshub.com/Cimbir/Store-Sales-Forecasting.mlflow/#/experiments/4/runs/33586c9faf314360b3189cae1fbdeab6

შემდეგი ნაბიჯია, რომ ავიღო overfitted მოდელები, რაც მივიღე და შევეცადო გავასწორო მათი მაღალი ვარიაცია regularization მეთოდების დამატებით. სასურველია, რომ train და validation score-ები დაახლოებით თანაბარი იყოს.

ვთქვათ ავიღოთ მოდელი `XGBoost_Obj_SquaredError_Estimator_100`

```
wmae_train 2887.015479419683
wmae_test 3707.861059151612
```


![Plot](plots/xgboost/Estimator_100_NoRegularization.png)

თუ დავაკვირდებით კონკრეტულად (Sale, Dept)=(1, 2) წყვილისთვის Weekly_Sales-ის prediction-ს, ვხედავთ, რომ ფაქტობრივად დაიზეპირა მოდელმა training set-ში არსებული pattern. უმჯობესი იქნება ამ შემთხვევაში, რომ უფრო ფრთხილი გზა ავარჩევინოთ XGBoost ხეების მაქსიმალური ზომის შემცირებით, L1 ან L2 რეგულარიზაციის დამატებით.

გადავწყვიტე საუკეთესო ჰიპერ-პარამეტრები მეპოვა GridSearch-ის საშუალებით შემდეგ range-ში:

```
max_depth in [3, 5]
alpha in [0, 3, 30, 100]
lambda in [1, 10, 20, 40, 100, 200]
```

მოცემული პარამეტრების ყველა შესაძლო ტრიპლეტისათვის გავწვრთენი მოდელი `Estimator=200`-ზე.

საუკეთესო მოდელი ამოვარჩიე მინიმალური სხვაობით WMAE_TRAIN-სა და WMAE_TEST-ს შორის შემდეგი პარამეტრებით:

```
{
    'objective': 'reg:squarederror', 
    'alpha': 100, 
    'lambda': 100, 
    'max_depth': 3
}
```

შეგვიძლია დავაკვირდეთ იმავე გრაფიკს (1, 2) მაღაზიისათვის და დავინახავთ, რომ ზემო გრაფიკთან შედარებით გაცილებით უფრო ნაკლებად მოძრაობს prediction მრუდი.

![Plot](plots/xgboost/Estimator_200_Regularization.png)

თუმცა ასევე გასათვალისწინებელია, რომ train და validation შედეგები ორივე გაუარესდა, მიუხედავად იმისა, რომ Estimator-ების რაოდენობა 100-დან 200-მდე გავზარდე.

```
mae_train 4842.940823694781
mae_test 5193.878655162364
wmae_train 5017.586377879966
wmae_test 5479.755364557295
```

### XGBoost_Estimator_500_L1_100_L2_100_Depth_3

https://dagshub.com/Cimbir/Store-Sales-Forecasting.mlflow/#/experiments/4/runs/80cfc38226a843f397b4f02eab48d648

ამ ექსპერიმენტში იმავე მოდელს ვიყენებ, რაც წინა ექსპერიმენტში თუმცა Estimator-ების რაოდენობას 500-მდე ვზრდი.

log-ებიდანაც ჩანს, რომ training-ის დროს test score ბოლოსკენ თითქმის აღარ იცვლება, თუმცა train score მაინც მცირდება

```
[495]	train-mae:4467.05844	train-wmae:4467.05811	test-mae:5067.59927	test-wmae:5067.59912
[496]	train-mae:4466.43190	train-wmae:4466.43164	test-mae:5066.75204	test-wmae:5066.75098
[497]	train-mae:4465.79447	train-wmae:4465.79443	test-mae:5066.39085	test-wmae:5066.39062
[498]	train-mae:4465.74906	train-wmae:4465.74951	test-mae:5068.73426	test-wmae:5068.73438
[499]	train-mae:4464.18536	train-wmae:4464.18555	test-mae:5068.05948	test-wmae:5068.05957
```

აქედან მივდივარ დასკვნამდე, რომ ჩემ მიერ შერჩეული feature-ების ლიმიტამდე მივედი ფაქტობრივად და გაუმჯობესება რთული იქნება overfit-ის გარეშე.

ამ მოდელისათვის შემდეგნაირად გამოიყურება feature importance ჰისტოგრამა:

![Plot](plots/xgboost/FeatureImportance_500_Regularized.png)

ჩემ მიერ შედგენილი feature-ებიდან, როგორც ჩანს, ყველაზე დიდი ყურადღება ექცევა Month-ს შემდეგ Day-ს და ბოლოს Year-ს.

მოდელს მიაჩნია, რომ Size ყველაზე მნიშვნელოვანი feature არის. დიდი ალბათობით იმიტომ, რომ შემოსავალი მჭიდროდაა კორელაციაში Size-ის მნიშვნელობასთან.


![Plot](plots/xgboost/SHAP_Beeswarm_500_Regularized.png)

ჩემს ჰიპოთეზას Size-ის შესახებ SHAP-ის Beeswarm plot-იც ადასტურებს. აქ ვხედავთ, რომ როდესაც Size-ის მნიშვნელობა მაღალია(წითლად რა ნაწილიცაა მონიშნული), ეს ეუბნება მოდელს, რომ Weekly შემოსავალიც მაღალი იქნება.

### XGBoost Feature Engineering 2

შემდეგი ნაბიჯია, რომ უკეთესი და უფრო კომპლექსური feature-ები შევარჩიო. ვიფიქრე, რომ Lag feature-ები დამემატებინა, რომლებიც მოდელს ეუბნებიან თუ რამდენიმე კვირის წინ რა იყო Weekly_Sales-ის მნიშვნელობა. ლოგიკურია, რომ მოდელისათვის ყველაზე კარგი იქნებოდა, რომ იცოდეს, მაგალითად, ბოლო 5 კვირის Weekly_Sales და ამით შეძლოს Prediction-ის გაკეთება. თუმცა ამის იმპლემენტაციას მივყავართ XGBoost-ის ლიმიტაციებამდე. მოდელი ძლიერად დამოკიდებული გახდებოდა წინა კვირების Sales feature-ებზე, თუმცა Inference-ის დროს არ იცი 2 წლიან range-ში არც ერთი მონაცემის Weekly_Sales. ვიფიქრე, რომ შესაძლებელი იყო დამეიმპლემენტირებინა Recursive Forecasting-ი, რომლის დროსაც ჯერ 1 მონაცემზე გააკეთებს prediction-ს, შეინახავს ამ prediction-ს და შემდეგი prediction-ისათვის გამოიყენებს ამ მნიშვნელობას. ანუ ყოველი მომდევნო prediction დამოკიდებული იქნებოდა წინა 5 prediction-ის მნიშვნელობაზე მაგალითად. ეს მიდგომა დავაიმპლემენტირე `CustomXGB` კლასში, მაგრამ პრაქტიკულად prediction step ძალიან ნელი გამოდგა და დაახლოებით ნახევარი საათი უნდოდა validation set-ზე გატარებას. ამ მიდგომით ყველანაირ ოპტიმიზაციას ვაბათილებდი, რასაც XGBoost მთავაზობს და საბოლოოდ გადავწყვიტე, რომ არ ღირდა ამ გზის არჩევა.

ალტერნატივა იქნებოდა, რომ lag-ები ამეღო წინა წლიდან, რადგან წლიური სეზონურობით გამოირჩევა მონაცემები. ეს იმ პრობლემას შექმნიდა, რომ მაქსიმუმ 1 წლით შეეძლებოდა მომავალში prediction და თუ რაიმე მიზეზის გამო ეს ახალი feature-ები NA იყო, შეიძლება performance მკვეთრად გაუარესებულიყო, თუმცა მაინც გადავწყვიტე ეს მიდგომა ამერჩია.

### XGBoost_LagFeats_Obj_SquaredError_Estimator_300_LR_0.1

https://dagshub.com/Cimbir/Store-Sales-Forecasting.mlflow/#/experiments/4/runs/f4777f720437444a90694bee891bd1cb

ამ მოდელში დავამატე შემდეგი lag-ები: 
`lags=[7*51, 7*52, 7*53]`

```
mae_train 2728.242011486581
mae_test 2807.606352043303
wmae_train 2824.445070695149
wmae_test 2985.1010091897083
```
ეს მიდგომა საკმაოდ წარმატებული გამოდგა ერთი შეხედვით. მაგალითად (1, 1) მაღაზიისათვის შეუძლია, რომ თითქმის უნაკლოდ გამოიცნოს validation set-ში რა მოხდება.

![Plot](plots/xgboost/LagFeats_11.png)

 ისიც აღსანიშნავია, რომ train და test ქულები ძალიან ახლოსაა. `AddLags()` კლასში მხოლოდ training set-ზე ხდება fit(), სადაც მოდელი იმახსოვრებს თითოეული თარიღისათვის მნიშვნელობებს და test set-ზე transform()-ს როდესაც ვაკეთებ, მასში ახალი lag feature-ები მხოლოდ ემატება training set-დან. იმაზე ვიზრუნე რომ validation set-დან არ გადმოჟონოს არაფერმა training-ში.


პრობლემა მაშინ ჩანს, როდესაც ძალიან შორს მივდივართ training set-დან და ამ დროს მოდელმა აღარ იცის რა გააკეთოს. prediciton მკვეთრად უარესდება, როგორც გრაფიკზე ჩანს.

![Plot](plots/xgboost/LagFeats_Average.png)

აქედან გამომდინარე Inference-ზე გაშვებამდე საუკეთესო მოდელი ჯობია მთლიან dataset-ზე გავწვრთნა.

Feature Importance-ის მხრივ, ჩემი დამატებული feature-ები ერთ-ერთი ყველაზე მნიშვნელოვანი გახდა წინა მოდელებთან შედარებით, რაც მოსალოდნელი იყო.

![Plot](plots/xgboost/LagFeats_FeatureImportance.png)


## LightGBM

https://dagshub.com/Cimbir/Store-Sales-Forecasting.mlflow/#/experiments/10

გადავწყვიტე იმავე მიდგომები გამომეცადა ასევე LightGBM-ზე და შემედარებინა XGBoost-თან performance-სა და სიჩქარეში.

### LightGBM Feature Engineering 1

თავდაპირველი ექსპერიმენტებისათვის XGBoost-ის მსგავსად აქაც მხოლოდ დავამატე `Year`, `Month`, `Day` და Train/Validation Split გავაკეთე ისევ `2011-09-01`-ში. იმისათვის, რომ მოდელმა დაითვალოს WMAE loss ფუნქცია, LightGBM-ის საკუთარ lgb.Dataset ობიექტში უნდა დამესეტა weight პარამეტრები თითოეული row-სთვის და შემდეგ ამერჩია MAE loss function. ასევე დავწერე custom WMAE მეტრიკა, რომელიც მოდელის training-ის შემდეგ გაეშევება და predict()-ს გამოიყენებს. ეს მარტივი Preprocessor MLflow-ზე დავლოგე, როგორც `LightGBM_Preprocessor_1`

### LightGBM Training 1

თავდაპირველად გადავწყვიტე ისევ SquaredError vs AbsoluteError გამომეცადა LightGBM-ისათვის და შემემოწმებინა ორივე Loss ფუნქციისათვის, როგორ გავლენას ახდენს Learning Rate-ის ცვლილება.


### LightGBM_Objective_MAE_Booster_1 - 500

https://dagshub.com/Cimbir/Store-Sales-Forecasting.mlflow/#/experiments/10/runs/f155f126733e4bc487ee34565e16ec44

ამ ექსპერიმენტების სერიაში default პარამეტრებზე და MAE loss ფუნქციაზე ვცადე, რომ შემეცვლა `num_booster_round` 1-დან 500-ის ჩათვლით და დავკვირვებოდი, როდის მიიღება overfit მოდელი.

Booster-ების რაოდენობის 1-დან 50-მდე გაზრდით მკვეთრად უმჯობესდება wmae_test დაახლოებით 1200-დან 5000-ის ფარგლებში. ანუ შეგვიძლია დავასკვნათ, რომ `num_booster_round<50` გვაძლევს underfitted მოდელს. 100-დან დაწყებული უკვე შესამჩნევი ხდება დიდი აცდენა train და test ქულებს შორის, თუმცა აღსანიშნავია, რომ LightGBM-ის overfit უფრო რთულია ვიდრე XGBoost-ის.

მაგალითად, რომ ავიღოთ უკიდურესი შემთხვევა `num_booster_round=500`, სადაც უკვე დაახლოებით 800 არის აცდენა train და test loss-ებს შორის:

```
wmae_train 3028.7613881699067
wmae_test 3842.1951623332334
```

ისევ მაღაზია (1, 1)-ის prediction vs observation plot-ს რომ დავაკვირდეთ, დავინახავთ, რომ როდესაც XGBoost გაცილებით უფრო flexibility-ის იჩენდა და პირდაპირ ერგებოდა training-ს, მაგალითად peak მნიშვნელობებს ყოველთვის აღწევდა, LightGBM გაცილებით უფრო თავშეკავებულია.

![Plot](plots/lightgbm/MAE_Booster_500_11.png)

პრობლემა იკვეთება, როდესაც შევხედავთ ნაკლებ შემოსავლიან მაღაზიებს, მაგალითად (30, 52), რომლის შემოსავალიც [0, 100] ფარგლებში მერყეობს.

![Plot](plots/lightgbm/MAE_Booster_500_3052.png)

მოდელი ამ დროს საკმაოდ არარეალისტური Sales-ის prediction-ს აკეთებს train set-ზეც კი. ჩემი აზრით, ეს იმიტომ ხდება, რომ იმდენად მცირეა მსგავსი დაბალშემოსავლიანი მაღაზიების რაოდენობა training set-ში, რომ მოდელი არაა encouraged საერთოდ დაისწავლოს ასეთი შემთხვევები. დიდი ალბათობით ამ პრობლემას გამოასწორებს Lag-ების დამატება მოდელისათვის და საშუალება ექნება შეხედოს წინა წლის მონაცემებს, ასევე შესაძლებელია, რომ თითოეული store-ისათვის დავითვალოთ საშუალო და მედიანა training მონაცემებიდან და ეგ დავუსეტოთ ახალ feature-ად.

### LightGBM_Objective_MAE_Booster_200_LR_0.05 - 0.01

ამ ექსპერიმენტებით აღმოვაჩინე, რომ learning rate-ის ცვლილება მკვეთრ გავლენას ახდენს საბოლოო შედეგზე. თუ შევამჩნევ, რომ მოდელი overfit-ს იწყებს მოცემული booster-ების რაოდენობით ყოველთვის შესაძლებელი იქნება, რომ learning rate default-თან შედარებით უფრო შევამცირო.

დაბალი learning_rate უფრო სტაბილურობას მიანიჭებს მოდელს, თუმცა ძალიან დაბალი learning_rate, მაგალითად 0.01 ჩვენი მოდელის შემთხვევაში პირიქით ხელს უშლის გაწვრთნის პროცესს:

```
mae_train 7232.159794621987
mae_test 7498.885142485389
wmae_train 7411.134066414457
wmae_test 7770.059105122366
```

Plot-ს თუ დავაკვირდებით, დავინახავთ, რომ lr=0.01-ზე და 200 booster-ზე ფაქტობრივად არაფერი არ დაისწავლა მოდელმა და prediction უბრალოდ სწორი ხაზია.

![Plot](plots/lightgbm/MAE_Booster_200_LowLR.png)

### LightGBM_Objective_Regression_Booster_1 - 200

https://dagshub.com/Cimbir/Store-Sales-Forecasting.mlflow/#/experiments/10/runs/27ef6d2c6b624ffdbc6a643e6f9ca043

ახლა რაც შეეხება შემთხვევას, როდესაც Loss ფუნქციად ავიღებთ SquaredError-ს, რომელსაც `regression` ჰქვია LightGBM-ში. XGBoost-ში უფრო კარგი შედეგები აჩვენა ამ loss ფუნქციამ და ამ შემთხვევაშიც საინტერესო იქნება.

ამ შემთხვევაშიც num_boost_round<50 მოდელები არის underfitted და მაღალი bias-ის მქონე. 1 booster-დან 50-მდე ჩამოსვლით test ქულა უმჯობესდება 14,000-დან დაახლოებით 5000-მდე მკვეთრად. 100 booster-დან დაწყებული 200-მდე მკვეთრი აცდენა იწყება train, test loss-ებს შორის. მაგალითად 200 booster-ზე გვაქვს:

```
mae_train 3263.835187022739
mae_test 3988.0463961320997
wmae_train 3285.4044149093324
wmae_test 4181.272388146837
```

და ეს შედეგი მოდი შევადაროთ MAE objective-ს ასევე 200 booster-ზე სხვა ყველა იმავე პარამეტრით:

```
mae_train 3642.6521061449553
mae_test 4217.9405385977925
wmae_train 3815.210534380747
wmae_test 4462.752770172549
```

ერთ-ერთი მნიშვნელოვანი განსხვავება MAE-სთან შედარებით ისაა, რომ SquaredError loss ცდილობს არსად არ ჰქონდეს დიდი ცდომილებები და ფასის spike-ებთან ახლოს უფრო ახლოს მიდის(შეგვიძლია შევადაროთ ზემოთ MAE-ს გრაფიკს იმავე მაღაზიისათვის).

![Plot](plots/lightgbm/SquaredError_Booster_200.png)

ერთი შეხედვით XGBoost-ის მსგავსად უფლო მალე მიიღება test/validation set-ზე უკეთესი შედეგები SquaredError loss-ით. დიდი ალბათობით, როგორც უკვე ვთქვი, უკეთესი გრადიენტის გამო ხდება ეს, თუმცა LightGBM-ში გადავწყვიტე, რომ გამოვიყენო მაინც MAE loss ფუნქცია. იმედი არის, რომ რადგან დიდი სხვაობების კვადრატებს არ მიაქცევს ყურადღებას და მხოლოდ სხვაობის მოდულს დააკვირდება, საბოლოოდ უფრო კარგი შედეგი ექნება WMAE-ზე.

### LightGBM Feature Engineering 2

შემდეგ ნაბიჯად გადავწყვიტე, რომ დამემატებინა Group Features. თითოეული (Store, Dept)-ისათვის დამეთვალა Min, Max, Mean, Median და ესენი ჩამეწერა ახალ feature-ებად. ამ სვლის მთავარი იდეა ის იყო, რომ მოდელს შეეძლებოდა გაანალიზოს თუ დაახლოებით რას უნდა ელოდოს თითოეული მაღაზიიდან და ძალიან უხეშ შეცდომებს აღარ დაუშვებდა. მაგალითად მაღაზიას რომლის შემოსავალი არასდროს აცდენია 1000-ს, უცნაური იქნება, რომ უცებ დიდი თანხა მიიღოს. ვიმედოვნებ, რომ prediction-ებს ჩასვამს [min, max] შუალედში და შეინარჩუნებს mean-სა და median-ს, შესაბამისად overfit-საც უპირისპირდება ფაქტობრივად ეს მიდგომა და უზრუნველყოფს, რომ ლოგიკურ ჩარჩოში იქნება prediction-ები.

სანამ ამ Feature-ებს დავაიმპლემენტირებდი ეს იყო Feature Importance Plot 200 Booster-იანი MAE objective მოდელისათვის:

![Plot](plots/lightgbm/SimpleFeats_200_Importance.png)

### LightGBM_GroupFeats_Objective_MAE_Booster_100

https://dagshub.com/Cimbir/Store-Sales-Forecasting.mlflow/#/experiments/10/runs/f39f49481a7443778385cff6c90d8fb6

Group Feature-ების იმპლემენტაციამ 100 Booster-იან მოდელზეც კი ძალიან კარგი შედეგი მომცა:

```
mae_train 2084.999485440237
mae_test 2504.721237950218
wmae_train 2260.9735262356508
wmae_test 2763.8684216768806
```


ახლა მოდელი ამჯობინებს, რომ რისკიანი prediction-ები არ გააკეთოს და მოცემული მონაცემების ფარგლებში დარჩეს, ზუსტად ისე, როგორც ვიმედოვნებდი რომ მოხდებოდა.

![Plot](plots/lightgbm/MAE_GroupFeats_12.png)



ამ მიდგომამ გამოასწორა პრობლემა მაღაზია (30, 52)-ისთვისაც(რომელიც ზემოთ იყო მოყვანილი), რომლისთვისაც მოდელი training set-ზეც კი ძალიან დიდ და არალოგიკურ prediction-ებს აკეთებდა.

ახლა მიუხედავად იმისა, რომ არ ცდილობს მოდელი მკვეთრი ცვლილებები დაიჭიროს, ვფიქრობ, უფრო კარგი შედეგია წინასთან შედარებით და თან overfit-ის საფრთხეც ნაკლებია.

![Plot](plots/lightgbm/MAE_GroupFeats_3052.png)

შეგვიძლია ჩავხედოთ Feature Importance ჰისტოგრამასაც და დავრწმუნდეთ, რომ მართლა ჩემი შემოღებული feature-ები იქცა ყველაზე მნიშვნელოვნად.

![Plot](plots/lightgbm/GroupFeats_100_Importance.png)

როგორც ვვარაუდობდი Mean-სა და Median-ს დიდი მნიშვნელობა ენიჭება, ასევე ყურადღებას აქცევს Min-სა და Max-ს რაც ძალიან კარგია.
